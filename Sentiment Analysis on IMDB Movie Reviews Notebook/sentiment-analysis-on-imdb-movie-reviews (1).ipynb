{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Project Overview","metadata":{}},{"cell_type":"markdown","source":"# üé¨ Sentiment Analysis on IMDB Movie Reviews\n\n## üìå Project Overview\nThis project applies **Natural Language Processing (NLP)** and **Machine Learning** techniques to classify IMDB movie reviews as **Positive (1)** or **Negative (0)**.  \nThe pipeline includes **data preprocessing, feature extraction, multiple model training, performance evaluation, and visualization** to identify the best-performing sentiment classification model.\n\n## üìä Dataset\n- **Source**: [IMDB Movie Review Dataset]  \n- **Size**: 50,000 labeled reviews  \n- **Features**:  \n  - `review`: text content of movie reviews  \n  - `sentiment`: target label (0 = Negative, 1 = Positive)  \n\n## ‚öôÔ∏è Project Features\n\n### üîç Data Preprocessing\n- Removed **HTML tags, punctuation, numbers, and special characters**  \n- Converted text to lowercase  \n- Applied **tokenization, stopword removal, and lemmatization**  \n- Balanced and shuffled dataset for fair training  \n\n### üß© Feature Extraction\n- **Bag-of-Words (CountVectorizer)**  \n- **TF-IDF Vectorization** for SVM model  \n\n### ü§ñ Machine Learning Models\n1. **Na√Øve Bayes**  \n   - Simple baseline model  \n   - Moderate accuracy, quick training  \n\n2. **Logistic Regression**  \n   - Strong linear model  \n   - Performed well with balanced precision/recall  \n\n3. **Support Vector Machine (SVM)**  \n   - Implemented with TF-IDF features  \n   - Tuned for performance on sampled dataset (10,000 reviews)  \n\n### üìä Model Evaluation\n- Metrics: **Accuracy, Precision, Recall, F1-Score, Confusion Matrix, ROC-AUC**  \n- Side-by-side comparison of all models  \n- Ranking system based on key metrics (Accuracy, F1, ROC-AUC)  \n\n## üìà Visualizations\n- üìå **Confusion Matrix Heatmap** (for best-performing model)  \n- üìå **Word Clouds** for Positive and Negative reviews  \n- üìå **Histograms** of review length (Positive vs Negative)  \n- üìå **Bar Chart & Histogram of Review Length Distribution** by categories (Short, Medium, Long)  \n\n## üèÜ Results & Best Model\n- **Na√Øve Bayes** ‚Üí Good baseline, fast but less accurate  \n- **Logistic Regression** ‚Üí Reliable, balanced performance  \n- **SVM** ‚Üí Delivered the **highest overall performance**  \n\n‚úÖ **Best Model Selected: Support Vector Machine (SVM)**  \n- Accuracy: ~**89%**  \n- Precision/Recall: High & consistent  \n- ROC-AUC: ~**90%** Strong classification power \n\n## üìå Conclusion\nThis project demonstrates the power of **text preprocessing + ML models** in sentiment classification.  \nThe **SVM model with TF-IDF features** proved to be the best choice for analyzing IMDB reviews, achieving strong accuracy and robust evaluation scores.\n","metadata":{}},{"cell_type":"markdown","source":"##  Load Training and Testing Data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,roc_auc_score\n\n#load csv\ndf=pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\nprint(\"\\nOriginal Data: \",df.head(5))\n\n#check for null values\nprint(df.isnull().sum())\n\n#convert sentiment to numeric\nle = LabelEncoder()\ndf['sentiment'] = le.fit_transform(df['sentiment']) #0 for negative and 1 for positive\nprint(\"\\nAfter Encoding\")\nprint(df.head(5))\n\n#shuffle the dataframe\ndf=df.sample(frac=1).reset_index(drop=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T02:27:03.603538Z","iopub.execute_input":"2025-09-21T02:27:03.603720Z","iopub.status.idle":"2025-09-21T02:27:09.727882Z","shell.execute_reply.started":"2025-09-21T02:27:03.603703Z","shell.execute_reply":"2025-09-21T02:27:09.726877Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preprocessing (Tokenization + Stopwords removal + Lemmatization)","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport re\n\n# Convert text to string\ndf['review'] = df['review'].astype(str)\n\n# Before preprocessing print data\nprint(\"\\nBefore Preprocessing\")\nprint(df.head(2))\n\n# Text preprocessing function\ndef preprocess_text(text):\n    # Remove HTML tags\n    text = re.sub('<.*?>', ' ', text)\n    \n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove punctuation, numbers, and special characters (keep only letters)\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    \n    # Remove extra spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    # Tokenization\n    tokens = text.split()\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    # Lemmatization\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n    \n    # Join tokens back into string\n    return \" \".join(tokens)\n\n# Apply preprocessing\ndf['review'] = df['review'].apply(preprocess_text)\n\n# After preprocessing print data\nprint(\"\\nAfter Preprocessing\")\nprint(df.head(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T02:30:06.464978Z","iopub.execute_input":"2025-09-21T02:30:06.465704Z","iopub.status.idle":"2025-09-21T02:30:51.516510Z","shell.execute_reply.started":"2025-09-21T02:30:06.465670Z","shell.execute_reply":"2025-09-21T02:30:51.515642Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Extraction & Model NaveBayes Prediction","metadata":{}},{"cell_type":"code","source":"#features\nX=df['review']\n\n#target\ny=df['sentiment']\n\n\n#convert text into numerical features\nvectorizer=CountVectorizer()\nX_counts=vectorizer.fit_transform(df['review'])\n\n#Train,test and split data\nX_train,X_test,y_train,y_test=train_test_split(X_counts,y,test_size=0.3,random_state=42)\n\n#nave bayes model\nmodel=MultinomialNB()\nmodel.fit(X_train,y_train)\n\n#prediction\ny_pred=model.predict(X_test)\nprint(\"===========Model Nave Bayes=============\")\nprint(\"News is predicted as : \",y_pred[:10])\n\n#metrics evaluation\nnb_acc=accuracy_score(y_test, y_pred)\nnb_pre=precision_score(y_test, y_pred)\nnb_rec=recall_score(y_test, y_pred)\nnb_f1=f1_score(y_test, y_pred)\nnb_cm=confusion_matrix(y_test, y_pred)\nnb_roc=roc_auc_score(y_test, y_pred)\n\nprint(\"==========================================\")\nprint(\"============Model Performance=============\")\nprint(\"Accuracy:\",nb_acc)\nprint(\"Precision:\",nb_pre)\nprint(\"Recall:\",nb_rec)\nprint(\"F1-Score:\",nb_f1)\nprint(\"Confusion Matrix:\\n\",nb_cm)\nprint(\"ROC-AUC:\",nb_roc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T02:31:10.191949Z","iopub.execute_input":"2025-09-21T02:31:10.192777Z","iopub.status.idle":"2025-09-21T02:31:15.125512Z","shell.execute_reply.started":"2025-09-21T02:31:10.192751Z","shell.execute_reply":"2025-09-21T02:31:15.124753Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Logistic Regression Prediction","metadata":{}},{"cell_type":"code","source":"#logistic regression\nreg=LogisticRegression(max_iter=1000)\nreg.fit(X_train,y_train)\nreg_pred=reg.predict(X_test)\n\n#predictions\nprint(\"===========Model Logistic Regression=============\")\nprint(\"News is predicted as : \",reg_pred[:10])\n\n#metrics evaluation\nlr_acc=accuracy_score(y_test, reg_pred)\nlr_pre=precision_score(y_test, reg_pred)\nlr_rec=recall_score(y_test, reg_pred)\nlr_f1=f1_score(y_test, reg_pred)\nlr_cm=confusion_matrix(y_test, reg_pred)\nlr_roc=roc_auc_score(y_test, reg_pred)\n\nprint(\"==========================================\")\nprint(\"============Model Performance=============\")\nprint(\"Accuracy:\",lr_acc)\nprint(\"Precision:\",lr_pre)\nprint(\"Recall:\",lr_rec)\nprint(\"F1-Score:\",lr_f1)\nprint(\"Confusion Matrix:\\n\",lr_cm)\nprint(\"ROC-AUC:\",lr_roc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T02:32:16.726161Z","iopub.execute_input":"2025-09-21T02:32:16.726519Z","iopub.status.idle":"2025-09-21T02:32:17.232757Z","shell.execute_reply.started":"2025-09-21T02:32:16.726494Z","shell.execute_reply":"2025-09-21T02:32:17.232041Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SVM Predictions","metadata":{}},{"cell_type":"code","source":"# STRATEGIC SAMPLING: Take a smaller representative sample \nsample_size = 10000  \ndf_sampled = df.sample(n=sample_size, random_state=42, replace=False)\n\n# Use this sampled dataframe for ALL models from now on\nX_sampled = df_sampled['review']\ny_sampled = df_sampled['sentiment']\n\nprint(f\"Working with a manageable sample of {sample_size} reviews for model comparison.\")\n\n# Convert text into numerical features\nvectorizer = TfidfVectorizer(max_features=5000)\nX_counts = vectorizer.fit_transform(X_sampled)\n\n# Train, test and split data\nX_train, X_test, y_train, y_test = train_test_split(X_counts, y_sampled, test_size=0.3, random_state=42)\n\n# SVM model\n\nsvm = SVC(random_state=42)  \nsvm.fit(X_train, y_train)\nsvm_pred = svm.predict(X_test)\n\n\n# Predictions\nprint(\"===========Model Support Vector Machine=============\")\nprint(\"First 10 predictions: \", svm_pred[:10])\nprint(\"Actual first 10 labels:\", y_test.values[:10])\n\n# Metrics evaluation\nsvm_acc = accuracy_score(y_test, svm_pred)\nsvm_pre = precision_score(y_test, svm_pred)\nsvm_rec = recall_score(y_test, svm_pred)\nsvm_f1 = f1_score(y_test, svm_pred)\nsvm_cm = confusion_matrix(y_test, svm_pred)\nsvm_roc = roc_auc_score(y_test, svm_pred)\n\nprint(\"==========================================\")\nprint(\"============Model Performance=============\")\nprint(f\"Accuracy: {svm_acc:.4f}\")\nprint(f\"Precision: {svm_pre:.4f}\")\nprint(f\"Recall: {svm_rec:.4f}\")\nprint(f\"F1-Score: {svm_f1:.4f}\")\nprint(\"Confusion Matrix:\\n\", svm_cm)\nprint(f\"ROC-AUC: {svm_roc:.4f}\")\n\n# Additional useful info\nprint(f\"\\nAdditional Info:\")\nprint(f\"Training samples: {X_train.shape[0]}\")\nprint(f\"Testing samples: {X_test.shape[0]}\")\nprint(f\"Number of features: {X_train.shape[1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T02:31:24.104728Z","iopub.execute_input":"2025-09-21T02:31:24.104985Z","iopub.status.idle":"2025-09-21T02:32:16.724746Z","shell.execute_reply.started":"2025-09-21T02:31:24.104963Z","shell.execute_reply":"2025-09-21T02:32:16.723730Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Comparison of all Models and choose the best one","metadata":{}},{"cell_type":"code","source":"results=pd.DataFrame({\n    \n\n    'Model':['Nave Bayes','Logistic Regression','Support Vector Machine'],\n    'Accuracy':[nb_acc,lr_acc,svm_acc],\n    'Precision':[nb_pre,lr_pre,svm_pre],\n    'Recall':[nb_rec,lr_rec,svm_rec],\n    'F1-Score':[nb_f1,lr_f1,svm_f1],\n    'Confusion Matrix':[nb_cm,lr_cm,svm_cm],\n    'ROC-AUC':[nb_roc,lr_roc,svm_roc]\n\n})\n\nprint(\"============== üìä Model Comparison Table ==============\")\nprint(results.to_string())\n\n# Rank models based on metrics (lower rank = better)\nmetrics_to_rank = [\"Accuracy\", \"F1-Score\", \"ROC-AUC\"]\nfor metric in metrics_to_rank:\n    results[f\"{metric}_Rank\"] = results[metric].rank(ascending=False)\n\n# Calculate overall rank\nresults[\"Overall_Rank\"] = results[[f\"{m}_Rank\" for m in metrics_to_rank]].sum(axis=1)\n\n# Sort by overall rank\nresults_sorted = results.sort_values(\"Overall_Rank\")\nprint(\"\\n============== üèÜ Ranked Models ==============\")\nprint(results_sorted.to_string())\n\n# Select best model\nbest_model = results_sorted.iloc[0][\"Model\"]\nprint(\"\\nüèÜ Best Model Selected:\", best_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T02:32:39.278008Z","iopub.execute_input":"2025-09-21T02:32:39.278326Z","iopub.status.idle":"2025-09-21T02:32:39.309903Z","shell.execute_reply.started":"2025-09-21T02:32:39.278306Z","shell.execute_reply":"2025-09-21T02:32:39.309215Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizations","metadata":{}},{"cell_type":"markdown","source":"## Confusion Matrix (According to SYM) for Best Model Accuracy","metadata":{}},{"cell_type":"code","source":"# Normalize to percentage\nsvm_cm_percent = svm_cm.astype(\"float\") / svm_cm.sum() * 100\n\nplt.figure(figsize=(7, 8))\nplt.imshow(svm_cm_percent, interpolation=\"nearest\", cmap=\"ocean\")\nplt.title(\"SVM - Confusion Matrix (%)\", fontsize=14, fontweight=\"bold\")\nplt.colorbar(label=\"Percentage\")\n\n# Axis labels\nplt.xticks([0, 1], [\"Predicted Legit (0)\", \"Predicted Phishing (1)\"], fontsize=10)\nplt.yticks([0, 1], [\"Actual Legit (0)\", \"Actual Phishing (1)\"], fontsize=10)\nplt.ylabel(\"Actual Class\", fontsize=12)\nplt.xlabel(\"Predicted Class\", fontsize=12)\n\n# Annotate cells with percentage\nfor i in range(svm_cm_percent.shape[0]):\n    for j in range(svm_cm_percent.shape[1]):\n        plt.text(j, i, f\"{svm_cm_percent[i, j]:.2f}%\", \n                 ha=\"center\", va=\"center\", color=\"black\", fontsize=11, fontweight=\"bold\")\n\nplt.tight_layout()\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T02:44:59.330569Z","iopub.execute_input":"2025-09-21T02:44:59.331093Z","iopub.status.idle":"2025-09-21T02:44:59.559662Z","shell.execute_reply.started":"2025-09-21T02:44:59.331037Z","shell.execute_reply":"2025-09-21T02:44:59.558851Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Wordcloud for Positive/Negative Reviews","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud\n\n# Get positive and negative reviews\npositive_reviews = df[df['sentiment']==1]['review']\nnegative_reviews = df[df['sentiment']==0]['review']\n\n# Create word clouds for positive and negative reviews\npositive_wordcloud = WordCloud(colormap='Greens').generate(' '.join(positive_reviews))  \nnegative_wordcloud = WordCloud(colormap='Reds').generate(' '.join(negative_reviews))\n\n\n# Plot word clouds\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(positive_wordcloud, interpolation='bilinear')\nplt.title('Positive Reviews')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(negative_wordcloud, interpolation='bilinear')\nplt.title('Negative Reviews')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T02:32:44.982377Z","iopub.execute_input":"2025-09-21T02:32:44.982679Z","iopub.status.idle":"2025-09-21T02:33:21.950866Z","shell.execute_reply.started":"2025-09-21T02:32:44.982659Z","shell.execute_reply":"2025-09-21T02:33:21.950041Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Histogram ‚Äì Review Length Distribution (Positive vs Negative Reviews)","metadata":{}},{"cell_type":"code","source":"\n\n# Calculate review lengths\ndf['review_length'] = df['review'].apply(lambda x: len(x.split())) #apply() lets you run a function on each element of a column\n#for each row of x in text column split the string into words and count them.\n\n# Separate lengths by sentiment\npos_lengths = df[df['sentiment']==1]['review_length']\nneg_lengths = df[df['sentiment']==0]['review_length']\n\n# Plot side-by-side histograms\nplt.figure(figsize=(12,5))\n\nplt.subplot(1, 2, 1)\nplt.hist(pos_lengths, bins=20, color='red', alpha=0.6, edgecolor='black')\nplt.title('Positive Reviews Length')\nplt.xlabel('Number of Words')\nplt.ylabel('Frequency')\n\nplt.subplot(1, 2, 2)\nplt.hist(neg_lengths, bins=20, color='purple', alpha=0.6, edgecolor='black')\nplt.title('Negative Reviews Length')\nplt.xlabel('Number of Words')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T02:34:05.315758Z","iopub.execute_input":"2025-09-21T02:34:05.316089Z","iopub.status.idle":"2025-09-21T02:34:06.047091Z","shell.execute_reply.started":"2025-09-21T02:34:05.316043Z","shell.execute_reply":"2025-09-21T02:34:06.046264Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Distribution of Review Length vs Word Count Categories","metadata":{}},{"cell_type":"markdown","source":"## Histogram","metadata":{}},{"cell_type":"code","source":"# Calculate review lengths\ndf['review_length'] = df['review'].apply(lambda x: len(x.split()))\ndf['short_review'] = df['review_length'].apply(lambda x: True if x < 50 else False) \ndf['medium_review']= df['review_length'].apply(lambda x: True if x >= 50 and x < 150 else False)\ndf['long_review'] = df['review_length'].apply(lambda x: True if x > 150 else False)\n\n#subplots for each review\nplt.figure(figsize=(14,6))\n\n#subplot for short reviews\nplt.subplot(1,3,1)\nplt.hist(df[df['short_review'] == True]['review_length'], bins=20, color='red', alpha=0.6, edgecolor='black')\nplt.title('Short Reviews Length')\nplt.xlabel('Number of Words')\nplt.ylabel('Frequency')\n\n#subplot for medium reviews\nplt.subplot(1,3,2)\nplt.hist(df[df['medium_review']==True]['review_length'],bins=20,color='yellow',alpha=0.6,edgecolor='black')\nplt.title('Medium Reviews Length')\nplt.xlabel('Number of Words')\nplt.ylabel('Frequency')\n\n#subplot for long reviews\nplt.subplot(1,3,3)\nplt.hist(df[df['long_review'] == True]['review_length'], bins=20, color='green', alpha=0.6, edgecolor='black')\nplt.title('Long Reviews Length')\nplt.xlabel('Number of Words')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T02:34:08.723843Z","iopub.execute_input":"2025-09-21T02:34:08.724177Z","iopub.status.idle":"2025-09-21T02:34:09.668163Z","shell.execute_reply.started":"2025-09-21T02:34:08.724153Z","shell.execute_reply":"2025-09-21T02:34:09.667342Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Bar Chart","metadata":{}},{"cell_type":"code","source":"\n# Review length categories\ndf['review_length'] = df['review'].apply(lambda x: len(x.split()))\ndf['review_type'] = pd.cut(df['review_length'],\n                           bins=[0, 50, 150, float('inf')],\n                           labels=['Short', 'Medium', 'Long'])\n\n# Count values for each category\ncounts = df['review_type'].value_counts().reindex(['Short', 'Medium', 'Long'])\n\n# Bar chart with border\nplt.figure(figsize=(8,6))\nbars = plt.bar(counts.index, counts.values, \n               color=['red', 'yellow', 'green'], \n               edgecolor='black', linewidth=1.5)\n\nplt.title('Distribution of Short, Medium, and Long Reviews')\nplt.xlabel('Review Type')\nplt.ylabel('Count')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T02:34:14.490611Z","iopub.execute_input":"2025-09-21T02:34:14.490888Z","iopub.status.idle":"2025-09-21T02:34:15.026996Z","shell.execute_reply.started":"2025-09-21T02:34:14.490870Z","shell.execute_reply":"2025-09-21T02:34:15.026113Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Submission.csv","metadata":{}},{"cell_type":"code","source":"# Save predictions of best model (logistic regression) for Kaggle submission\n\nsubmission = pd.DataFrame({\n    \"id\": range(len(svm_pred)),\n    \"sentiment\": svm_pred\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"‚úÖ submission.csv file generated!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T02:42:52.643994Z","iopub.execute_input":"2025-09-21T02:42:52.644859Z","iopub.status.idle":"2025-09-21T02:42:52.653704Z","shell.execute_reply.started":"2025-09-21T02:42:52.644825Z","shell.execute_reply":"2025-09-21T02:42:52.652771Z"}},"outputs":[],"execution_count":null}]}