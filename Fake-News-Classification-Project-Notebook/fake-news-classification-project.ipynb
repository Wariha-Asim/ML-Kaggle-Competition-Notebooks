{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8165591,"sourceType":"datasetVersion","datasetId":4831777}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Project Overview","metadata":{}},{"cell_type":"markdown","source":"# üì∞ Fake News Classification Project  \n\n## üìå Project Overview  \nThe rapid spread of misinformation has become one of the biggest challenges in the digital age.  \nThis project aims to **classify news articles as either Fake (0) or Real (1)** using machine learning models.  \nWe combine **data preprocessing, feature extraction, and multiple ML algorithms** to identify which model performs best for the task.  \n\n\n\n## üìä Dataset  \nWe use two datasets:  \n\n- **Fake.csv** ‚Üí Contains fake news articles.  \n- **True.csv** ‚Üí Contains real news articles.  \n\nBoth datasets were combined into a single dataframe with a new **label column**:  \n- `0` ‚Üí Fake News  \n- `1` ‚Üí Real News  \n\nAfter merging and shuffling, preprocessing steps were applied including:  \n- Merging `title` and `text` columns.  \n- Lowercasing all text.  \n- Removing punctuation, numbers, and symbols.  \n- Lemmatization with **WordNet Lemmatizer**.  \n- Stopword removal (NLTK).  \n\n\n\n## ‚öôÔ∏è Project Features  \n‚úîÔ∏è **Text Preprocessing:** Cleaning, tokenization, lemmatization, stopword removal.  \n‚úîÔ∏è **Feature Extraction:** `CountVectorizer` used for Bag-of-Words representation.  \n‚úîÔ∏è **Model Training:** Implemented and compared:  \n   - **Naive Bayes**  \n   - **Logistic Regression**  \n   - **Random Forest**  \n‚úîÔ∏è **Evaluation Metrics:** Accuracy, Precision, Recall, F1-Score, ROC-AUC.  \n‚úîÔ∏è **Visualizations:** Top words, histograms, and word clouds for Fake vs Real news.  \n\n\n\n## üèÜ Results & Best Model  \nAfter comparing models on multiple metrics:  \n\n| Model               | Accuracy | Precision | Recall | F1-Score | ROC-AUC |\n|---------------------|----------|-----------|--------|----------|---------|\n| Logistic Regression | **0.98** | **0.98**  | **0.98** | **0.98** | **0.98** |\n| Random Forest       | 0.97     | 0.97      | 0.97   | 0.97     | 0.97    |\n| Naive Bayes         | 0.94     | 0.95      | 0.92   | 0.93     | 0.94    |\n\nüìå **Best Performing Model:**  \n**Logistic Regression** with the highest **Accuracy, F1-score, and ROC-AUC (‚âà98%)**, making it the most reliable choice for Fake News Detection.  \n\n\n","metadata":{}},{"cell_type":"markdown","source":"##  Load Training and Testing Data","metadata":{}},{"cell_type":"code","source":"#importing libraries\n# Data handling\nimport pandas as pd\nimport numpy as np\n\n# Text preprocessing\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n# Feature extraction\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n# Model training\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model evaluation\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,roc_auc_score\n\n#visualization\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\ndf_fake_news=pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv')\ndf_real_news=pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')\n\n#adding label columns in both datatsets\ndf_fake_news['label'] = 0\ndf_real_news['label'] = 1\n\n#concat both dataframes\ndf=pd.concat([df_fake_news,df_real_news],axis=0)\n\n#find missing values in new combined dataframe\nprint(df.isnull().sum())\nprint(df.shape)\n\n#shuffle dataframe\ndf=df.sample(frac=1).reset_index(drop=True)\nprint(df.head(5))\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T20:44:53.319812Z","iopub.execute_input":"2025-09-19T20:44:53.320725Z","iopub.status.idle":"2025-09-19T20:44:54.973035Z","shell.execute_reply.started":"2025-09-19T20:44:53.320691Z","shell.execute_reply":"2025-09-19T20:44:54.971489Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Step 1: Merge title + text\ndf['text'] = df['title'] + \" \" + df['text']\n\n# Step 2: Lowercase conversion\ndf['text'] = df['text'].str.lower()\n\n# Step 3: Remove punctuation, numbers, symbols\ndf['text'] = df['text'].str.replace('[^a-zA-Z]', ' ', regex=True)\n\n# Step 4: Preprocessing function using NLTK stopwords\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\ndef preprocess(text):\n    # Split words manually\n    tokens = text.split()\n    # Remove NLTK stopwords and lemmatize\n    tokens = [lemmatizer.lemmatize(w) for w in tokens if w.lower() not in stop_words]\n    return \" \".join(tokens)\n\n# Step 5: Print before vs after\nprint(\"Before preprocessing:\\n\", df['text'].head(5))\n\n# Apply preprocessing\ndf['text'] = df['text'].apply(preprocess)\n\nprint(\"\\nAfter preprocessing:\\n\", df['text'].head(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T20:44:54.974596Z","iopub.execute_input":"2025-09-19T20:44:54.975047Z","iopub.status.idle":"2025-09-19T20:45:55.093243Z","shell.execute_reply.started":"2025-09-19T20:44:54.975015Z","shell.execute_reply":"2025-09-19T20:45:55.092041Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Extraction & Model NaveBayes Prediction","metadata":{}},{"cell_type":"code","source":"\n#features and target\nX=df['text']\ny=df['label']\n\n#convert text into numerical features\nvectorizer=CountVectorizer()\nX_counts=vectorizer.fit_transform(df['text'])\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_counts, y, test_size=0.3, random_state=42)\n\n# Model\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)\n\n# Prediction\ny_pred = model.predict(X_test)\nprint(\"===========Model Nave Bayes=============\")\nprint(\"News is predicted as : \",y_pred[:10])\n\n#metrics evaluation\nnb_acc=accuracy_score(y_test, y_pred)\nnb_pre=precision_score(y_test, y_pred)\nnb_rec=recall_score(y_test, y_pred)\nnb_f1=f1_score(y_test, y_pred)\nnb_cm=confusion_matrix(y_test, y_pred)\nnb_roc=roc_auc_score(y_test, y_pred)\n\nprint(\"==========================================\")\nprint(\"============Model Performance=============\")\nprint(\"Accuracy:\",nb_acc)\nprint(\"Precision:\",nb_pre)\nprint(\"Recall:\",nb_rec)\nprint(\"F1-Score:\",nb_f1)\nprint(\"Confusion Matrix:\\n\",nb_cm)\nprint(\"ROC-AUC:\",nb_roc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T20:45:55.094209Z","iopub.execute_input":"2025-09-19T20:45:55.094525Z","iopub.status.idle":"2025-09-19T20:46:04.063650Z","shell.execute_reply.started":"2025-09-19T20:45:55.094501Z","shell.execute_reply":"2025-09-19T20:46:04.062801Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Logistic Regression Predictions","metadata":{}},{"cell_type":"code","source":"#logistic regression\nreg = LogisticRegression(max_iter=1000)\nreg.fit(X_train,y_train)\nreg_pred=reg.predict(X_test)\n\n#predictions\nprint(\"===========Model Logistic Regression=============\")\nprint(\"News is predicted as : \",reg_pred[:10])\n\n#metrics evaluation\nlr_acc=accuracy_score(y_test, reg_pred)\nlr_pre=precision_score(y_test, reg_pred)\nlr_rec=recall_score(y_test, reg_pred)\nlr_f1=f1_score(y_test, reg_pred)\nlr_cm=confusion_matrix(y_test, reg_pred)\nlr_roc=roc_auc_score(y_test, reg_pred)\n\nprint(\"==========================================\")\nprint(\"============Model Performance=============\")\nprint(\"Accuracy:\",lr_acc)\nprint(\"Precision:\",lr_pre)\nprint(\"Recall:\",lr_rec)\nprint(\"F1-Score:\",lr_f1)\nprint(\"Confusion Matrix:\\n\",lr_cm)\nprint(\"ROC-AUC:\",lr_roc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T21:03:47.043089Z","iopub.execute_input":"2025-09-19T21:03:47.043408Z","iopub.status.idle":"2025-09-19T21:03:54.747469Z","shell.execute_reply.started":"2025-09-19T21:03:47.043385Z","shell.execute_reply":"2025-09-19T21:03:54.746674Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Random Forest Predictions","metadata":{}},{"cell_type":"code","source":"#random forest\nrf=RandomForestClassifier()\nrf.fit(X_train,y_train)\nrf_pred=rf.predict(X_test)\n\n#predictions\nprint(\"===========Model Random Forest=============\")\nprint(\"News is predicted as : \",rf_pred[:10])\n\n#metrics evaluation\nrf_acc=accuracy_score(y_test, rf_pred)\nrf_pre=precision_score(y_test, rf_pred)\nrf_rec=recall_score(y_test, rf_pred)\nrf_f1=f1_score(y_test, rf_pred)\nrf_cm=confusion_matrix(y_test, rf_pred)\nrf_roc=roc_auc_score(y_test, rf_pred)\n\nprint(\"==========================================\")\nprint(\"============Model Performance=============\")\nprint(\"Accuracy:\",rf_acc)\nprint(\"Precision:\",rf_pre)\nprint(\"Recall:\",rf_rec)\nprint(\"F1-Score:\",rf_f1)\nprint(\"Confusion Matrix:\\n\",rf_cm)\nprint(\"ROC-AUC:\",rf_roc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T20:46:10.882616Z","iopub.execute_input":"2025-09-19T20:46:10.882893Z","iopub.status.idle":"2025-09-19T20:48:02.440852Z","shell.execute_reply.started":"2025-09-19T20:46:10.882872Z","shell.execute_reply":"2025-09-19T20:48:02.439937Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Comparison between Models to decide which performs best ","metadata":{}},{"cell_type":"code","source":"# Comparing Results of Logistic Regression, RF and Nave Bayes \nresults = pd.DataFrame({\n    \"Model\": [\n        \"Logistic Regression \",\n        \"Random Forest\",\n        \"Naive Bayes\"\n    ],\n    \"Accuracy\": [\n        lr_acc, rf_acc, nb_acc\n       \n    ],\n    \"Precision\": [\n        lr_pre, rf_pre, nb_pre\n\n    ],\n        \n    \"Recall\":[\n        lr_rec, rf_rec, nb_rec\n    ],\n    \"F1_score\": [\n        lr_f1, rf_f1, nb_f1\n    ],\n    \"ROC_AUC\":[\n        lr_roc, rf_roc, nb_roc\n    ]\n})\n\nprint(\"============== üìä Model Comparison Table ==============\")\nprint(results.to_string())\n\n# ===================== Ranking Models =====================\n# Rank based on Accuracy, F1 Score, and ROC-AUC\nresults[\"Acc_Rank\"] = results[\"Accuracy\"].rank(ascending=False)\nresults[\"F1_Rank\"] = results[\"F1_score\"].rank(ascending=False)\nresults[\"ROC_AUC_Rank\"] = results[\"ROC_AUC\"].rank(ascending=False)\n\n# Calculate overall rank (lower = better)\nresults[\"Overall_Rank\"] = results[[\"Acc_Rank\",\"F1_Rank\",\"ROC_AUC_Rank\"]].sum(axis=1)\n\n# Sort models by rank\nresults_sorted = results.sort_values(\"Overall_Rank\")\nprint(\"\\n============== üèÜ Ranked Models ==============\")\nprint(results_sorted.to_string())\n\n# Best model\nbest_model = results_sorted.iloc[0][\"Model\"]\nprint(\"\\nüèÜ Best Model Selected:\", best_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T20:48:02.441820Z","iopub.execute_input":"2025-09-19T20:48:02.442103Z","iopub.status.idle":"2025-09-19T20:48:02.470407Z","shell.execute_reply.started":"2025-09-19T20:48:02.442081Z","shell.execute_reply":"2025-09-19T20:48:02.469380Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizations","metadata":{}},{"cell_type":"markdown","source":"## Top 20 Words in Fake News","metadata":{}},{"cell_type":"code","source":"#Top 20 words in Fake News\nfake_words = \" \".join(df[df['label'] == 0]['text']).split()\nfake_counter = Counter(fake_words)\ntop_fake = fake_counter.most_common(20)\n\n# top_real already list of tuples [('word1', count1), ('word2', count2), ...]\nwords_fake = [w for w, c in top_fake]   # x-axis\ncounts_fake = [c for w, c in top_fake]  # y-axis\n\nplt.figure(figsize=(10,6))\nplt.bar(words_fake, counts_fake, color='red')\nplt.xticks(rotation=45, ha='right')\nplt.title(\"Top 20 Words in Fake News\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T20:48:02.471402Z","iopub.execute_input":"2025-09-19T20:48:02.471859Z","iopub.status.idle":"2025-09-19T20:48:04.525447Z","shell.execute_reply.started":"2025-09-19T20:48:02.471820Z","shell.execute_reply":"2025-09-19T20:48:04.524575Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Top 20 Words in Real News","metadata":{}},{"cell_type":"code","source":"# Top 20 words in Real News\nreal_words = \" \".join(df[df['label'] == 1]['text']).split()\nreal_counter = Counter(real_words)\ntop_real = real_counter.most_common(20)\n\n# top_real already list of tuples [('word1', count1), ('word2', count2), ...]\nwords_real = [w for w, c in top_real]   # x-axis\ncounts_real = [c for w, c in top_real]  # y-axis\n\nplt.figure(figsize=(10,6))\nplt.bar(words_real, counts_real, color='green')\nplt.xticks(rotation=45, ha='right')\nplt.title(\"Top 20 Words in Real News\")\nplt.ylabel(\"Frequency\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T20:48:04.526630Z","iopub.execute_input":"2025-09-19T20:48:04.526933Z","iopub.status.idle":"2025-09-19T20:48:06.195170Z","shell.execute_reply.started":"2025-09-19T20:48:04.526907Z","shell.execute_reply":"2025-09-19T20:48:06.193993Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Histogram: Article Length (Number of Words)","metadata":{}},{"cell_type":"code","source":"\ndf['word_count'] = df['text'].apply(lambda x: len(x.split()))\n\n# Plot histogram for fake and real news\nplt.figure(figsize=(10,5))\nplt.hist(df[df['label'] == 0]['word_count'], bins=30, alpha=0.6, label='Fake News', color='pink')\nplt.hist(df[df['label'] == 1]['word_count'], bins=30, alpha=0.6, label='Real News', color='purple')\nplt.xlabel(\"Number of Words per Article\")\nplt.ylabel(\"Number of Articles\")\nplt.title(\"Article Length Distribution: Fake vs Real News\", fontsize=16)\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T20:48:06.196230Z","iopub.execute_input":"2025-09-19T20:48:06.196608Z","iopub.status.idle":"2025-09-19T20:48:07.166810Z","shell.execute_reply.started":"2025-09-19T20:48:06.196585Z","shell.execute_reply":"2025-09-19T20:48:07.165777Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Wordcloud for Real/Fake News","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud\n\n# Get positive and negative reviews\npositive_reviews = df[df['label']==1]['text']\nnegative_reviews = df[df['label']==0]['text']\n\n# Create word clouds for positive and negative reviews\npositive_wordcloud = WordCloud(colormap='PuRd').generate(' '.join(positive_reviews))  \nnegative_wordcloud = WordCloud(colormap='plasma').generate(' '.join(negative_reviews))\n\n\n# Plot word clouds\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(positive_wordcloud, interpolation='bilinear')\nplt.title('Real News')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(negative_wordcloud, interpolation='bilinear')\nplt.title('Fake News')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T20:55:12.472378Z","iopub.execute_input":"2025-09-19T20:55:12.473084Z","iopub.status.idle":"2025-09-19T20:56:09.986846Z","shell.execute_reply.started":"2025-09-19T20:55:12.473057Z","shell.execute_reply":"2025-09-19T20:56:09.985588Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Submission.csv  \nA `submission.csv` file has been generated using the **Logistic Regression model**,  \nas it achieved the **best performance with 98% accuracy** on the validation set.  \nThis file contains the predicted labels (0 = Fake, 1 = Real) for the Kaggle test dataset.","metadata":{}},{"cell_type":"code","source":"# Save predictions of best model (logistic regression) for Kaggle submission\n\nsubmission = pd.DataFrame({\n    \"id\": range(len(reg_pred)),\n    \"sentiment\": reg_pred\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"‚úÖ submission.csv file generated!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T21:07:15.741947Z","iopub.execute_input":"2025-09-19T21:07:15.743045Z","iopub.status.idle":"2025-09-19T21:07:15.767790Z","shell.execute_reply.started":"2025-09-19T21:07:15.743017Z","shell.execute_reply":"2025-09-19T21:07:15.766801Z"}},"outputs":[],"execution_count":null}]}