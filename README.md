# ML Kaggle Competition Notebooks

This repository contains solutions for two Kaggle machine learning competitions, showcasing different approaches to structured data problems.

## ğŸ† Competitions

### 1. ğŸš¢ Titanic: Machine Learning from Disaster
**Notebook**: [Titanic Survival Prediction](https://github.com/Wariha-Asim/ML-Kaggle-Competition-Notebooks/blob/main/titanic-competition.ipynb)  
**Kaggle**: [Competition Entry](https://www.kaggle.com/code/waarihaasim/ml-titanic-competition-notebook)

**Key Takeaways**:
- Binary classification problem predicting passenger survival
- Random Forest achieved best performance (80.4% accuracy)
- Key insights: Women, children, and higher-class passengers had better survival rates
- Comprehensive data preprocessing and feature engineering

### 2. ğŸ  House Prices: Advanced Regression Techniques
**Notebook**: [House Price Prediction](https://github.com/Wariha-Asim/ML-Kaggle-Competition-Notebooks/blob/main/House%20Prices%20-%20Advanced%20Regression%20Techniques/house-prices-advanced-regression-techniques.ipynb)  
**Kaggle**: [Competition Entry](https://www.kaggle.com/code/waarihaasim/house-prices-advanced-regression-notebook)

**Key Takeaways**:
- Regression problem predicting home prices in Ames, Iowa
- XGBoost demonstrated superior performance (RÂ²: 0.911, MAE: $16,788)
- Handled 79 features with comprehensive preprocessing
- Insight: XGBoost baseline outperformed tuned models, showing strong default configurations

## ğŸ› ï¸ Technical Skills Demonstrated
- Data preprocessing & cleaning
- Feature engineering and selection
- Multiple ML algorithms (Random Forest, XGBoost, Logistic Regression, Decision Trees)
- Hyperparameter tuning and model evaluation
- Data visualization and insight extraction

## ğŸ‘©â€ğŸ’» Author
**Waariha Asim**
- Kaggle: [@waarihaasim](https://www.kaggle.com/waarihaasim)
- GitHub: [@Wariha-Asim](https://github.com/Wariha-Asim)

## ğŸ“„ License
This project is open source and available under the MIT License.
